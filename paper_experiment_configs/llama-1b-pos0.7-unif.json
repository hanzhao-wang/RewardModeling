{
    "deepspeed": "configs/state1.json",
    "model_name": "meta-llama/Llama-3.2-1B-Instruct",
    "per_device_train_batch_size": 8,
    "per_device_eval_batch_size": 16,
    "gradient_accumulation_steps": 16,
    "learning_rate": 1e-5,
    "weight_decay": 1e-3,
    "bf16": true,
    "num_train_epochs": 10,
    "output_path": "./bt_models",
    "gradient_checkpointing": true,
    "optim": "paged_adamw_32bit",
    "lr_scheduler_type": "cosine",
    "max_length": 2048,
    "save_every_steps": 999999,
    "eval_every_steps": 32,
    "use_lora": false,
    "trainer_type": "oraclece",
    "label_type": "original",
    "train_set_path": "statdata/prefer_skywork_Skywork/Skywork-Reward-Gemma-2-27B-v0.2",
    "seed": 42,
    "diff_rescaling_factor": 1,
    "warmup_ratio": 0.1,
    "selected_pos_ratio": 0.7,
    "select_method": "fixed"
}